# ROS Demo - Beverage Detection Pipeline

Learning ROS2 by building a perception pipeline for retail robotics's shelf-stocking robots.

**Status: Working!** Camera â†’ detector â†’ depth â†’ tracker pipeline complete. BoT-SORT tracking with trajectory visualization.

## Quick Start (Linux PC with NVIDIA GPU)

```bash
# 1. Clone and setup
git clone <repo>
cd ros-perception-demo
cp .env.example .env
# Edit .env for your paths

# 2. Build and run
docker compose build
docker compose up -d
docker exec -it ros_demo bash

# 3. Inside container: build ROS2 workspace
cd /ros_ws
colcon build
source install/setup.bash

# 4. Run nodes (in separate terminals)
# Terminal 1: Camera
ros2 run perception_demo camera_node --ros-args -p source:=webcam
# Or: -p source:=sim (static test images)

# Terminal 2: Detector (detection only, no tracking)
ros2 run perception_demo detector_node --ros-args -p model:=yolo

# Terminal 2 (alt): Tracker (detection + persistent IDs)
ros2 run perception_demo tracker_node --ros-args -p tracker:=botsort
# Or: -p tracker:=bytetrack

# Terminal 3: Depth (optional)
ros2 run perception_demo depth_node --ros-args -p method:=midas

# Terminal 4: Visualize
ros2 run rqt_image_view rqt_image_view
# â†’ Select: /tracks/image (with trajectory trails) or /detections/image or /depth/image
```

## Architecture

### ROS2 Node Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     /camera/image_raw   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     /detections
â”‚   camera_node   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  detector_node   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶
â”‚                 â”‚    sensor_msgs/Image    â”‚                  â”‚   Detection2DArray
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                                           â”‚
        â”‚ source:=                                  â”‚ model:=
        â”‚ â€¢ sim (static images) âœ“                   â”‚ â€¢ yolo (YOLOv8) âœ“
        â”‚ â€¢ webcam (USB camera) âœ“                   â”‚
        â”‚ â€¢ gazebo (future)                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     /tracks
        â”‚                                    â”‚  tracker_node    â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶
        â”‚                                    â”‚ (with trail viz) â”‚   Detection2DArray
        â”‚                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   + track IDs
        â”‚                                           â”‚ tracker:=
        â”‚                                           â”‚ â€¢ botsort (default) âœ“
        â”‚                                           â”‚ â€¢ bytetrack âœ“
        â”‚                                           â”‚
        â”‚                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                                    â”‚   depth_node     â”‚ â†’ /depth/image
        â”‚                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                                           â”‚ method:=
        â”‚                                           â”‚ â€¢ midas (monocular) âœ“
        â”‚                                           â”‚ â€¢ realsense (future)
        â”‚
        â”‚                                    Future nodes:
        â”‚                                    â€¢ pose_node (6DoF pose estimation)
        â”‚                                    â€¢ segmentation_node
        â”‚
                                                    â”‚ /tracks/image
                                                    â–¼
                                            (annotated image with trajectory trails)
```

**YOLOv8**: Auto-downloads on first run (~6MB), runs on PyTorch + CUDA.

### Full Pipeline Vision (retail robotics-style)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              PERCEPTION                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  camera_node â”€â”€â–¶ detector_node â”€â”€â–¶ depth_node â”€â”€â–¶ tracking_node             â”‚
â”‚       â”‚              â”‚                 â”‚               â”‚                     â”‚
â”‚   RGB image     2D bboxes          depth map      object IDs                â”‚
â”‚                      â”‚                 â”‚               â”‚                     â”‚
â”‚                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚                     â”‚
â”‚                               â–¼                        â”‚                     â”‚
â”‚                        pose_node â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                            â”‚                                                 â”‚
â”‚                      6DoF object pose                                        â”‚
â”‚                            â”‚                                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                            â–¼                                                 â”‚
â”‚                     grasp_node                                               â”‚
â”‚                          â”‚                                                   â”‚
â”‚                   grasp points                                               â”‚
â”‚                          â”‚                                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                          â–¼                          PLANNING & CONTROL       â”‚
â”‚                    planner_node â”€â”€â–¶ controller_node â”€â”€â–¶ robot                â”‚
â”‚                          â”‚               â”‚                                   â”‚
â”‚                    motion plan      joint commands                           â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Current:  âœ… camera_node, detector_node, depth_node, tracker_node
Next:     ğŸ”² Gazebo simulation (Panda arm + shelf world)
Future:   â¬œ pose_node, segmentation_node, grasp_node
Later:    â¬œ planner_node, controller_node

Gazebo Roadmap:
  v1: Panda arm + simple shelf + simulated camera (current goal)
  v2: Custom TX-style SCARA arm + convenience store shelf with real product models
```

### Development Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      git push      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MacBook Pro   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚     Gitea       â”‚
â”‚   (code editor) â”‚                    â”‚   (git server)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                               â”‚
                                               â”‚ git pull
                                               â–¼
                                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                       â”‚      Linux PC (Pop!_OS + RTX)       â”‚
                                       â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                                       â”‚  â”‚   Docker Container            â”‚  â”‚
                                       â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
                                       â”‚  â”‚  â”‚ ROS2 + PyTorch + CUDA   â”‚  â”‚  â”‚
                                       â”‚  â”‚  â”‚ camera_node             â”‚  â”‚  â”‚
                                       â”‚  â”‚  â”‚ perception_node         â”‚  â”‚  â”‚
                                       â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
                                       â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                                       â”‚           â”‚                         â”‚
                                       â”‚           â”‚ X11 (GUI)               â”‚
                                       â”‚           â–¼                         â”‚
                                       â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
                                       â”‚      â”‚ Monitor â”‚ â† Gazebo/RViz      â”‚
                                       â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
                                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                               â”‚
                                               â”‚ Future: same code runs on
                                               â–¼
                                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                       â”‚  Robot (Jetson) â”‚
                                       â”‚  Real cameras   â”‚
                                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**The magic:** Same ROS2 code runs in simulation (Docker) and on real robot. Just swap camera source.

### Visualization

View detections in real-time:

```bash
# Inside container, Terminal 3:
ros2 run rqt_image_view rqt_image_view
# Select topic: /detections/image
```

Or echo detection messages:
```bash
ros2 topic echo /detections
```

---

## ROS Core Concepts

### What is ROS?
- **Robot Operating System** - not actually an OS, but a middleware/framework
- Provides communication between processes (nodes)
- Industry standard for robotics software
- Think of it like Flask/Django for web â†’ ROS for robots

### Key Concepts

| Concept | Description | Example |
|---------|-------------|---------|
| **Node** | A separate OS process that does one thing well | `detection_node`, `camera_node` |
| **Topic** | Named channel for messages (pub/sub) | `/camera/image`, `/detections` |
| **Message** | Data structure sent over topics | `sensor_msgs/Image`, `geometry_msgs/Pose` |
| **Publisher** | Node that sends messages to a topic | Camera publishes images |
| **Subscriber** | Node that receives messages from a topic | Detector subscribes to images |
| **Service** | Request/response (synchronous) | "Take a photo" â†’ returns image |
| **Action** | Long-running task with feedback | "Navigate to shelf" â†’ progress updates |

### ROS1 vs ROS2

| | ROS1 | ROS2 |
|---|---|---|
| Python API | `rospy` | `rclpy` |
| C++ API | `roscpp` | `rclcpp` |
| Master | Required (`roscore`) | No master (DDS) |
| Real-time | Limited | Better support |
| Status | Legacy (ends 2025) | Current standard |

We'll use **ROS2 Humble** (LTS, supported until 2027).

### Nodes in Detail - Are They Really Separate Processes?

**Yes! Each node is a separate OS process.**

```bash
# When you run nodes, you can see them as separate processes:
$ ps aux | grep ros
user  1234  python3 detection_node    # Process 1
user  1235  python3 camera_node       # Process 2
user  1236  arm_controller_node       # Process 3 (could be C++)
```

**In production robots:**
- Nodes can be **Python** (`rclpy`) or **C++** (`rclcpp`)
- C++ often used for performance-critical nodes (control loops, real-time)
- Python often used for perception, high-level logic
- Nodes can run on **same machine** or **distributed across multiple computers**
- Communication via **DDS** (Data Distribution Service) - handles networking automatically

**Example: Real robot setup**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     Network      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Jetson (on robot)  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  Workstation (GPU)  â”‚
â”‚  - motor_control    â”‚                  â”‚  - perception_node  â”‚
â”‚  - sensor_drivers   â”‚                  â”‚  - planning_node    â”‚
â”‚  - arm_controller   â”‚                  â”‚  - visualization    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

DDS handles message passing across the network transparently.

**Why separate processes?**
- **Isolation** - one node crashes, others keep running
- **Modularity** - swap out perception node without touching arm control
- **Distributed** - spread across multiple computers
- **Language flexibility** - mix Python and C++ nodes

### Common Message Types

```
sensor_msgs/Image          # Camera images (RGB, depth)
sensor_msgs/PointCloud2    # 3D point clouds (LiDAR, depth camera)
geometry_msgs/Pose         # Position (x,y,z) + orientation (quaternion)
geometry_msgs/Twist        # Velocity (linear + angular)
std_msgs/String            # Simple string
std_msgs/Float32           # Simple float
```

### TF (Transforms)

Coordinate frame transforms - essential for perception:
- Where is the object relative to the camera?
- Where is the camera relative to the robot base?
- Where is the robot relative to the world?

```
world
  â””â”€â”€ robot_base
        â””â”€â”€ arm_link_1
              â””â”€â”€ arm_link_2
                    â””â”€â”€ camera_link
```

`tf2` library lets you query: "Where is object X in the robot_base frame?"

---

## If You've Built a Distributed System Before, You Already Know ROS

**Example: Quant trading system architecture = ROS architecture**

| Quant Trading System | ROS Equivalent |
|----------------------|----------------|
| Separate processes (Python/Golang/Rust/C++) | Nodes |
| Redis pub/sub | Topics (via DDS) |
| Websocket connector process | Camera driver node |
| Strategy/live trading process | Planning/control node |
| Data processing process | Perception node |
| Message queues | Topics with message types |

If you've built microservices or distributed systems, you already understand ROS conceptually.

### Redis Pub/Sub vs DDS (ROS2)

| Aspect | Redis Pub/Sub | DDS (ROS2 Topics) |
|--------|---------------|-------------------|
| Architecture | Central broker (Redis server) | Peer-to-peer (no broker) |
| Discovery | Manual (connect to Redis) | Auto-discovery on network |
| QoS (Quality of Service) | Basic | Rich (reliability, deadline, liveliness) |
| Real-time | No guarantees | Designed for real-time |
| Message types | Strings/bytes (you serialize) | Strongly typed (defined .msg files) |
| Use case | General purpose | Robotics, defense, aerospace |
| Failure mode | Redis dies = all dead | No single point of failure |

**Key DDS features for robotics:**
- **Reliability**: "Deliver this message even if network hiccups" vs "best effort"
- **Deadline**: "Warn me if message doesn't arrive within 100ms"
- **Liveliness**: "Detect if a node crashed"
- **History**: "Keep last N messages for late subscribers"

**In practice:** DDS is more robust for safety-critical robotics. Redis is fine for trading/web apps where occasional message loss is acceptable.

### How Perception Fits in ROS

```
Camera Driver Node                    Your Perception Node                 Planning Node
     â”‚                                       â”‚                                  â”‚
     â”‚ publishes                             â”‚ subscribes                       â”‚ subscribes
     â–¼                                       â–¼                                  â–¼
/camera/image_raw â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ runs YOLO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ /detections
                                             â”‚
                                             â”‚ publishes
                                             â–¼
                                      DetectionArray
                                      - class: "coca_cola"
                                      - bbox: [x, y, w, h]
                                      - confidence: 0.95
                                      - position_3d: [x, y, z]
```

## Project Structure

```
ros-perception-demo/
â”œâ”€â”€ README.md
â”œâ”€â”€ docker-compose.yml        # Container config (GPU, volumes, X11)
â”œâ”€â”€ .env.example              # Template for local paths
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ Dockerfile            # ROS2 Humble + PyTorch + CUDA
â”œâ”€â”€ src/
â”‚   â””â”€â”€ perception_demo/      # ROS2 Python package
â”‚       â”œâ”€â”€ package.xml
â”‚       â”œâ”€â”€ setup.py
â”‚       â””â”€â”€ perception_demo/
â”‚           â””â”€â”€ nodes/
â”‚               â”œâ”€â”€ camera/   # Camera sources
â”‚               â”‚   â”œâ”€â”€ sim.py      # Static test images
â”‚               â”‚   â””â”€â”€ webcam.py   # USB webcam
â”‚               â”œâ”€â”€ depth/    # Depth estimation
â”‚               â”‚   â””â”€â”€ midas.py    # Monocular (MiDaS)
â”‚               â”œâ”€â”€ detector/ # Object detection
â”‚               â”‚   â””â”€â”€ yolo.py     # YOLOv8
â”‚               â””â”€â”€ tracker/  # Object tracking
â”‚                   â””â”€â”€ yolo_tracker.py  # BoT-SORT / ByteTrack
â”œâ”€â”€ data/
â”‚   â””â”€â”€ test_images/          # Test images
â””â”€â”€ models/                   # Model weights (*.pt gitignored)
```

## Setup

### Prerequisites
- Linux PC with NVIDIA GPU (tested: Pop!_OS 22.04, RTX 2060)
- Docker with NVIDIA Container Toolkit
- Git

### Environment Variables (.env)

```bash
# External data path for large files (not in git)
ROS_DEMO_DATA=/mnt/your-drive/ros-perception-demo-data

# X11 display for GUI (check with `echo $DISPLAY`)
DISPLAY=:0
```

### GUI Access (Gazebo, RViz)

```bash
# Allow Docker to access X11 display
xhost +local:docker

# Then docker compose up will show GUI on your monitor
```

## Learning Path

### Day 1: ROS Basics
- [ ] Understand nodes, topics, pub/sub
- [ ] Run example nodes in Docker
- [ ] Use `ros2 topic list`, `ros2 topic echo`

### Day 2: Build Camera Node
- [ ] Create a node that publishes test images
- [ ] Learn `sensor_msgs/Image` format
- [ ] Use `cv_bridge` to convert OpenCV â†” ROS

### Day 3: Build Detection Node
- [ ] Subscribe to camera topic
- [ ] Run YOLOv8 inference
- [ ] Publish detection results

### Day 4: Integration & Review
- [ ] Run full pipeline
- [ ] Review ROS concepts for project
- [ ] Practice explaining the system

## ROS2 Cheat Sheet

```bash
# List all nodes
ros2 node list

# List all topics
ros2 topic list

# See messages on a topic
ros2 topic echo /camera/image

# Topic info (message type, publishers, subscribers)
ros2 topic info /camera/image

# Message structure
ros2 interface show sensor_msgs/msg/Image

# Run a node
ros2 run <package> <node>

# Launch multiple nodes
ros2 launch <package> <launch_file.py>
```

---

## ROS vs Gazebo vs Isaac Sim

These are different tools that work together:

| Tool | What it does | Analogy |
|------|--------------|---------|
| **ROS** | Communication between nodes | The "nervous system" |
| **Gazebo** | Physics simulation | The "virtual world" |
| **Isaac Sim** | NVIDIA's simulator (GPU-accelerated) | Premium "virtual world" |
| **RViz** | Visualization (no physics) | The "debug viewer" |

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Gazebo (Physics Sim)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Robot     â”‚  â”‚   Shelf     â”‚  â”‚   Beverages         â”‚  â”‚
â”‚  â”‚   Model     â”‚  â”‚   Model     â”‚  â”‚   (physics objects) â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                                                   â”‚
â”‚         â”‚ Simulated sensors (camera, depth, lidar)          â”‚
â”‚         â–¼                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              ROS2 Topics                            â”‚    â”‚
â”‚  â”‚  /camera/image  /depth/image  /robot/joint_states   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚     Your Perception Node      â”‚
              â”‚  (same code as real robot!)   â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**The magic:** Your perception code doesn't know if images come from real camera or Gazebo. Same code works in simulation and real robot.

### Gazebo on Mac / Docker

| Option | Works? | Notes |
|--------|--------|-------|
| Native Mac | âŒ | Not supported |
| Docker + X11 | âš ï¸ | Possible but tricky (XQuartz) |
| Docker + VNC | âœ… | Works well, access via browser |
| Docker headless | âœ… | No GUI, but can save images/videos |

**For project:** Gazebo is optional. Focus on ROS basics first. If time permits, we can add Gazebo with VNC.

### Gazebo Setup (Optional - Stretch Goal)

We'll add to docker-compose.yml:
- Gazebo Fortress (works with ROS2 Humble)
- VNC server for GUI access
- Simple shelf + beverages world

```yaml
# Access Gazebo GUI at: http://localhost:6080
```

---

## retail robotics-Relevant Simulation Ideas

If we add Gazebo, we could simulate:

1. **Convenience store shelf** with beverage products
2. **TX SCARA robot arm** (or simplified version)
3. **Camera mounted on arm** publishing images
4. **Your perception node** detecting products
5. **Grasp planning** based on detections

This would be impressive to mention in practice: *"I built a ROS2 + Gazebo simulation of a shelf-stocking scenario to prepare for this role."*

---

## References

- [ROS2 Humble Docs](https://docs.ros.org/en/humble/)
- [ROS2 Tutorials](https://docs.ros.org/en/humble/Tutorials.html)
- [cv_bridge (ROS â†” OpenCV)](https://github.com/ros-perception/vision_opencv)
- [Gazebo Sim](https://gazebosim.org/)
- [ROS2 + Gazebo Integration](https://gazebosim.org/docs/harmonic/ros2_integration)

---

## Appendix

### RQT Tools

**RQT** = ROS Qt GUI toolkit. Useful tools:

| Tool | Command | Purpose |
|------|---------|---------|
| Image viewer | `ros2 run rqt_image_view rqt_image_view` | View camera/detection images |
| Topic monitor | `ros2 run rqt_topic rqt_topic` | See all topics and messages |
| Node graph | `ros2 run rqt_graph rqt_graph` | Visualize node connections |
| Console | `ros2 run rqt_console rqt_console` | View logs from all nodes |

### Docker on Mac (Alternative Setup)

If you don't have a Linux PC with GPU, you can run on Mac with limitations:

**Docker on Mac = Linux inside a container.**

- ROS is built for Ubuntu - native Mac install is painful
- Docker runs a lightweight Linux environment
- Your Mac stays clean, ROS runs in isolated Linux container
- Industry standard - retail robotics likely uses Docker too
- Easy to delete and start fresh
- Reproducible environment (like Python venv but for entire OS)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Your Mac (macOS)          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚     Docker Container (Linux)  â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚   ROS2 + PyTorch        â”‚  â”‚  â”‚
â”‚  â”‚  â”‚   Your perception nodes â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Limitations on Mac:**
- No NVIDIA GPU â†’ CPU inference only (slower)
- GUI (Gazebo/RViz) requires XQuartz or VNC
- Apple Silicon (M1/M2) needs ARM images

**Recommendation:** Use Linux PC with NVIDIA GPU for best experience.
